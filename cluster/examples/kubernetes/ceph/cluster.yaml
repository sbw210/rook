#################################################################################################################
# Production 클러스터에 대한 공통 설정으로 rook-ceph 클러스터에 대한 설정을 정의
# 사용 가능한 Raw Devices가 있는 모든 노드가 Ceph 클러스터에 사용됨 (3개이상의 노드가 필요)

# For example, to create the cluster:
#   kubectl create -f crds.yaml -f common.yaml -f operator.yaml
#   kubectl create -f cluster.yaml
#################################################################################################################

apiVersion: ceph.rook.io/v1
kind: CephCluster
metadata:
  name: rook-ceph
  namespace: rook-ceph 
spec:
  cephVersion:
    # Ceph daemon Pod(mon, mgr, osd, mds, rgw)를 시작하는데 사용되는 컨테이너 이미지.
    # v13 is mimic, v14 is nautilus, and v15 is octopus.
    # RECOMMENDATION: 프로덕션 환경에서는,일반 v14 대신 특정 버전 Tag를 사용할 것. 
    # 지정하지 않으면 최신 release를 가져와서 클러스터 내에서 다른 버전이 실행될 수 있음. https://hub.docker.com/r/ceph/ceph/tags/ 에서 사용가능한 Tag를 참조할 것.
    # 좀더 정확하게 하고싶다면, 다음과 같이 타임스탬프 태그를 사용할 수 있음. ceph/ceph:v15.2.8-20201217
    # 이 Tag에는 새로운 Ceph 버전이 포함되어있지 않을 수 있으며, 취약점을 줄이는 기본 운영체제의 보안 수정사항만 포함될 수 있습니다.
    image: ceph/ceph:v15.2.8
    # 지원되지 않는 Ceph버전을 허용할 지 여부. 현재 `nautilus`와 `octopus` 가 지원됨.
    # `pacific`과 같은 향후 버전에서는 이를 `true`로 설정해주어야함.
    # 프로덕션 환경에서는 true로 설정하지 마시오!
    allowUnsupported: false
  # Configuration File이 유지될 호스트의 경로 지정.
  # 중요!!: 클러스터를 다시 설치하는 경우에는 각 호스트에서 이 디렉터리를 삭제해주어야함!!!. 그렇지 않으면 mons가 새 클러스터에서 시작되지 않음
    dataDirHostPath: /var/lib/rook
  # 점검에 실패하더라도 업그레이드를 계속진행할지 여부.
  # 이것은 Ceph의 상태가 저하될 수 있으며, 업그레이드를 권장하지 않지만, 소유자의 결정에 따를 수 있음을 의미
  # Rook의 Ceph 업그레이드 프로세스를 이해하려면 다음 링크를 참조 = https://rook.io/docs/rook/master/ceph-upgrade.html#ceph-version-upgrades
  skipUpgradeChecks: false
  # 업그레이드 중 PG가 not clean일 경우 계속진행할지 여부 결정
  continueUpgradeAfterChecksEvenIfNotHealthy: false
  # WaitTimeoutForHealthyOSDInMinutes는 operator가 업그레이드 또는 재시작을 위해 OSD를 중지할 때 까지 대기하는 시간(분)을 정의.
  # 시간이 초과되고 OSD가 중지할 수 없는 경우, operator는 현재 OSD에 대한 업그레이드를 건너뛰고 다음 OSD를 진행
  # 만약 `continueUpgradeAfterChecksEvenIfNotHealthy`가 `false`인 경우 `continueUpgradeAfterChecksEvenIfNotHealthy`가 `true`이면, 
  # operator는 시간초과 후 중지할 수 없더라도 OSD 업그레이드를 계속진행함.
  # 하지만 `skipUpgradeChecks` 가 `true`일 경우, 이 시간제한이 적용되지 않음. 
  # 기본 시간제한 값은 10분
  waitTimeoutForHealthyOSDInMinutes: 10
  mon:
    # 시작할 mons의 수를 설정. 홀수이어야하며(쿼럼때문에), 일반적으로 3을 권장
    count: 3
    # mons는 고유한(unique) 노드에 있어야함(한 노드에 mons 2개 이상 X). 그래서 프로덕션 환경일 경우, 최소 3개의 노드가 권장됨.
    # Mons는 데이터 손실이 허용되는 테스트환경의 동일한 노드에서만 허용이되어야함 (즉, 테스트환경에서만 동일한노드에 2개이상 배포해도 ok)
    allowMultiplePerNode: false
  mgr:
    modules:
    # 이 목록에 여러 모듈을 포함할 필요는 없음. "대시보드" 및 "모니터링" 모듈은 클러스터 CR의 다른 설정에 의해 이미 활성화 되어있음!
    - name: pg_autoscaler
      enabled: true
  # 클러스터 상태를 보기위해 Ceph Dashboard 활성화(enable)
  dashboard:
    enabled: true
    # 하위 경로에서 대시보드 제공 (reverse proxy를 통해 대시보드에 엑세스할 때 유용함)
    # urlPrefix: /ceph-dashboard
    # 아래 포트에서 대시보드를 제공
    # port: 8443
    # SSL을 사용하여 대시보드를 제공
    ssl: true
  # 클러스터에 대한 프로메테우스 Alert 활성화 여부
  monitoring:
    # Prometheus가 사전 설치되어 있어야함.
    enabled: false
    # prometheusRule을 배포할(deploy) 네임스페이스. 비어있는 경우, 클러스터의 네임스페이스가 사용됨.
    # 추천(recommanded):
    # 단일 Rook-Ceph 클러스터가 있는 경우, rulesNamespace를 클러스터와 동일한 네임스페이스로 설정하거나 비워둡니다.
    # 동일한 k8s클러스터에 여러개의 rook-ceph 클러스터가 있는 경우, 동일한 네임스페이스(이상적으로는 prometheus가 있는 네임스페이스)를 
    # 모든 클러스터에 대한 rulesNamespace를 설정합니다. 그렇지 않으면 중복된 alert를 받게됨.
    rulesNamespace: rook-ceph
  network:
    # enable host networking
    #provider: host
    # EXPERIMENTAL: enable the Multus network provider
    #provider: multus
    #selectors:
      # The selector keys는 `public` and `cluster`이어야함.
      # 구성에 따라 operator는 다음을 수행.
      #   1. `public` selector key 만 지정하면 public_network 및 cluster_network Ceph 설정이 해당 인터페이스에서 수신 대기
      #   2. `public`과 `cluster` selector keys 모두 지정되면 첫 번째 키는 'public_network' flag를 가리키고 두번째 키는 'cluster_network'를 가리킴
      #
      # In order to work, each selector value must match a NetworkAttachmentDefinition object in Multus
      #
      #public: public-conf --> NetworkAttachmentDefinition object name in Multus
      #cluster: cluster-conf --> NetworkAttachmentDefinition object name in Multus
    # Provide internet protocol version. IPv6, IPv4 or empty string are valid options. Empty string would mean IPv4
    #ipFamily: "IPv6"
  # edaemon crash 수집을 위해 crashCollector를 활성화
  crashCollector:
    disable: false
    # daysToRetain의 주석처리를 제거하여 지정된 일수 보다 오래된 ceph crash항목을 제거하시오.
    #daysToRetain: 30
  # log collector를 활성화하면 daemon이 파일에 로그온하고 로테이트(rotate)됨.
  # logCollector:
  #   enabled: true
  #   periodicity: 24h # SUFFIX는 'h' (시간) or 'd' (일)일 수 있음.
  # automate [data cleanup process](https://github.com/rook/rook/blob/master/Documentation/ceph-teardown.md#delete-the-data-on-hosts) in cluster destruction.
  cleanupPolicy:
    # cluster cleanup은 데이터를 파괴하므로 확인이 필요.
    # 제거 중에 호스트의 모든 rook data를 삭제하려면 확인을 "yes-really-destroy-data"로 설정해야함.
    # 이 값은 클러스터가 삭제되려고 할 때만 설정해야함. '확인'이 설정되면 rook은 즉시 클러스터 구성을 중지하고 delete 명령만 기다림.
    # 빈 문자열이 설정되면 rook은 제거 중에 호스트의 data를 삭제하지 않음.
    confirmation: ""
    # sanitizeDisks는 클러스터 삭제시 OSD disk를 삭제하기 위한 설정을 나타냄.
    sanitizeDisks:
      # method는 전체 disk를 삭제해야하는지 아니면 단순히 ceph의 메타데이터를 삭제하는지를 나타냄
      # 두 경우 모두 재설치가 가능
      # 가능한 선택지는'complete' 또는 'quick' (기본값)
      method: quick
      # dataSource는 disk에 쓸 임의의 바이트를 가져올 위치를 나타냄.
      # 가능한선택지는 'zero' (기본값) 또는 'random'
      # random sources를 사용하면 시스템에서 엔트로피를 소비하고 zero source보다 훨씬 더 많은 시간이 걸립니다.
      dataSource: zero
      # iteration은 default (1) 대신 N번 덮어씀.
      # 정수값을 받음.
      iteration: 1
    # allowUninstallWithVolumes은 제거(uninstall) 수행 방법을 정의
    # 'true'로 설정하면 cephCluster 삭제는 PV가 삭제될 때 까지 기다리지 않습니다.
    allowUninstallWithVolumes: false
  # Kubernetes에서 다양한 서비스를 예약할 위치를 제어하려면 아래 배치구성 섹션을 사용할 것
  # 'all'아래의 예시는 'role=storage-node'로 label이 지정된 kubernetes 노드에서 예약된 모든 서비스를 가지며,  'storage-node' key로 taint를 toleration함.
#  placement:
#    all:
#      nodeAffinity:
#        requiredDuringSchedulingIgnoredDuringExecution:
#          nodeSelectorTerms:
#          - matchExpressions:
#            - key: role
#              operator: In
#              values:
#              - storage-node
#      podAffinity:
#      podAntiAffinity:
#      topologySpreadConstraints:
#      tolerations:
#      - key: storage-node
#        operator: Exists
# 위의 배치 정보는 mon, osd 및 mgr 구성요서에 대해서도 지정할 수 있음.
#    mon:
# Monitor deployments에는 동일한 노드에 모니터 중복배치를 방지하기 위해 anti-affinity 규칙이 포함될 수 있음.
# HostNetwork를 사용하거나 AllowMultiplePerNode가 false인 경우 필수 규칙!
# 그렇지않으면 이 anti-affinity는 가중치가 50인 기본규칙으로 적용됨
#    osd:
#    mgr:
#    cleanup:
  annotations:
#    all:
#    mon:
#    osd:
#    cleanup:
#    prepareosd:
# mgr 주석이 설정되지 않은 경우, prometheus scrape 주석이 기본적으로 설정됨.
#    mgr:
  labels:
#    all:
#    mon:
#    osd:
#    cleanup:
#    mgr:
#    prepareosd:
  resources:
# 여기 설정된 request 및 limits는 mgr pod가 1개의 cpu 코어 절반과 1GB 메모리를 사용하도록 허용함 (샘플)
#    mgr:
#      limits:
#        cpu: "500m"
#        memory: "1024Mi"
#      requests:
#        cpu: "500m"
#        memory: "1024Mi"
# 위의 예제의 request/limits는 mon 및 osd 구성요소에도 추가할 수 있음.
#    mon:
#    osd:
#    prepareosd:
#    crashcollector:
#    logcollector:
#    cleanup:
  # 꺼져있는(안전하게 제거할수있는) OSD를 자동으로 제거하는 옵션
  removeOSDsIfOutAndSafeToRemove: false
#  priorityClassNames:
#    all: rook-ceph-default-priority-class
#    mon: rook-ceph-mon-priority-class
#    osd: rook-ceph-osd-priority-class
#    mgr: rook-ceph-mgr-priority-class
  storage: # cluster level storage configuration and selection (클러스터에 사용될 storage로 무엇을 쓸것인지)
    useAllNodes: true
    useAllDevices: true
    #deviceFilter:
    config:
      # crushRoot: "custom-root" # CRUSH MAP에 기본이 아닌 root label을 지정하시오
      # metadataDevice: "md0" # 비회전 storage를 지정하여 ceph-volume이 이를 bluestore의 블록 db 장치로 사용하도록.
      # databaseSizeMB: "1024" # 디스크가 100 GB보다 더 작은 경우 주석 해제
      # journalSizeMB: "1024"  # 디스크가 20GB 이하인경우 주석 해제
      # osdsPerDevice: "1" # 이 값은 노드 or 장치 레벨에서 재정의 가능
      # encryptedDevice: "true" # 기본값은 false임
# 개별노드및해당 구성도 지정할수있지만, 그렇게 하려면 위의 'useALLNodes'를 false로 설정해야함.
# 그러면 아래에 정의된 노드만 스토리지 리소스로 사용됨. 각 노드의 'name'필드는 'kubernetes.io/hostname' 레이블과 일치해야함.(k8s에서 정의된 호스트네임과 동일하게)
#    nodes:
#    - name: "172.17.4.201"
#      devices: # 스토리지에 사용할 특정 장치를 각 노드에 지정할 수 있음.
#      - name: "sdb"
#      - name: "nvme01" # 고성능 장치에서 여러 OSD를 만들 수 있음.
#        config:
#          osdsPerDevice: "5"
#      - name: "/dev/disk/by-id/ata-ST4000DM004-XXXX" # 전체 udev path를 사용하여 장치를 지정할 수 있음
#      config: # configuration can be specified at the node level which overrides the cluster level config
#        storeType: filestore
#    - name: "172.17.4.301"
#      deviceFilter: "^sd."
  # 업그레이드 또는 fencing 중에 데몬 중지를 구성하는 섹션
  disruptionManagement:
    # true인 경우, operator는 OSD, Mon, RGW 및 MDS 데몬에 대한 PodDisruptionBudgets를 만들고 관리함.
    # OSD PDB는 https://github.com/rook/rook/blob/master/design/ceph/ceph-managed-disruptionbudgets.md에 설명된 방식으로 동적으로 관리됨.
    # operator는 기본적으로 OSD 제거를 차단하고 drains가 확인되면 안전하게 차단해제함.
    managePodBudgets: true
    # drain할 때 `region / zone / host`와 같은 전체 failureDomain이`noout` (기본 DOWN / OUT 간격 외에)에 유지되는 기간을 결정하는 기간 (분)입니다.
    # 이는 'managePodBudgets'가 'true'인 경우에만 관련됨. 기본값은 30분
    osdMaintenanceTimeout: 30
    # Drain이 완료되고 OSD가 다시 활성화 된 후, operator가 PG가 정상(active+clean)이 될 때 까지 기다리는 시간(분).
    # Operator는 제한시간이 초과되면 다음 drain을 계속함. `managePodBudgets`이`true`일 경우에만 작동.
    # 값이 없거나 0은 작업자가 배치 그룹이 정상상태가 될 때 까지 기다렸다가 다음 drain을 차단함을 의미
    pgHealthCheckTimeout: 0
    # True이면 Operator가 MachineDisruptionBudgets를 생성하고 관리하여 클러스터가 정상일 때만 OSD가 차단되도록 함.
    # OpenShift에서만 사용할 수 있음
    manageMachineDisruptionBudgets: false
    # MachineDisruptionBudgets를 감시할 네임스페이스
    machineDisruptionBudgetNamespace: openshift-machine-api

  # healthChecks (헬스체크)
  # Valid values for daemons are 'mon', 'osd', 'status'
  healthCheck:
    daemonHealth:
      mon:
        disabled: false
        interval: 45s
      osd:
        disabled: false
        interval: 60s
      status:
        disabled: false
        interval: 60s
    # pod liveness probe 변경, 모든 mon,mgr,osd 데몬에서 작동
    livenessProbe:
      mon:
        disabled: false
      mgr:
        disabled: false
      osd:
        disabled: false
