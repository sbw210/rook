#################################################################################################################
# Production 클러스터에 대한 공통 설정으로 rook-ceph 클러스터에 대한 설정을 정의
# 사용 가능한 Raw Devices가 있는 모든 노드가 Ceph 클러스터에 사용됨 (3개이상의 노드가 필요)

# For example, to create the cluster:
#   kubectl create -f crds.yaml -f common.yaml -f operator.yaml
#   kubectl create -f cluster.yaml
#################################################################################################################

apiVersion: ceph.rook.io/v1
kind: CephCluster
metadata:
  name: rook-ceph
  namespace: rook-ceph 
spec:
  cephVersion:
    # Ceph daemon Pod(mon, mgr, osd, mds, rgw)를 시작하는데 사용되는 컨테이너 이미지.
    # v13 is mimic, v14 is nautilus, and v15 is octopus.
    # RECOMMENDATION: 프로덕션 환경에서는,일반 v14 대신 특정 버전 Tag를 사용할 것. 
    # 지정하지 않으면 최신 release를 가져와서 클러스터 내에서 다른 버전이 실행될 수 있음. https://hub.docker.com/r/ceph/ceph/tags/ 에서 사용가능한 Tag를 참조할 것.
    # 좀더 정확하게 하고싶다면, 다음과 같이 타임스탬프 태그를 사용할 수 있음. ceph/ceph:v15.2.8-20201217
    # 이 Tag에는 새로운 Ceph 버전이 포함되어있지 않을 수 있으며, 취약점을 줄이는 기본 운영체제의 보안 수정사항만 포함될 수 있습니다.
    image: ceph/ceph:v15.2.8
    # 지원되지 않는 Ceph버전을 허용할 지 여부. 현재 `nautilus`와 `octopus` 가 지원됨.
    # `pacific`과 같은 향후 버전에서는 이를 `true`로 설정해주어야함.
    # 프로덕션 환경에서는 true로 설정하지 마시오!
    allowUnsupported: false
  # Configuration File이 유지될 호스트의 경로 지정.
  # 중요!!: 클러스터를 다시 설치하는 경우에는 각 호스트에서 이 디렉터리를 삭제해주어야함!!!. 그렇지 않으면 mons가 새 클러스터에서 시작되지 않음
    dataDirHostPath: /var/lib/rook
  # 점검에 실패하더라도 업그레이드를 계속진행할지 여부.
  # 이것은 Ceph의 상태가 저하될 수 있으며, 업그레이드를 권장하지 않지만, 소유자의 결정에 따를 수 있음을 의미
  # Rook의 Ceph 업그레이드 프로세스를 이해하려면 다음 링크를 참조 = https://rook.io/docs/rook/master/ceph-upgrade.html#ceph-version-upgrades
  skipUpgradeChecks: false
  # 업그레이드 중 PG가 not clean일 경우 계속진행할지 여부 결정
  continueUpgradeAfterChecksEvenIfNotHealthy: false
  # WaitTimeoutForHealthyOSDInMinutes는 operator가 업그레이드 또는 재시작을 위해 OSD를 중지할 때 까지 대기하는 시간(분)을 정의.
  # 시간이 초과되고 OSD가 중지할 수 없는 경우, operator는 현재 OSD에 대한 업그레이드를 건너뛰고 다음 OSD를 진행
  # 만약 `continueUpgradeAfterChecksEvenIfNotHealthy`가 `false`인 경우 `continueUpgradeAfterChecksEvenIfNotHealthy`가 `true`이면, 
  # operator는 시간초과 후 중지할 수 없더라도 OSD 업그레이드를 계속진행함.
  # 하지만 `skipUpgradeChecks` 가 `true`일 경우, 이 시간제한이 적용되지 않음. 
  # 기본 시간제한 값은 10분
  waitTimeoutForHealthyOSDInMinutes: 10
  mon:
    # 시작할 mons의 수를 설정. 홀수이어야하며(쿼럼때문에), 일반적으로 3을 권장
    count: 3
    # mons는 고유한(unique) 노드에 있어야함(한 노드에 mons 2개 이상 X). 그래서 프로덕션 환경일 경우, 최소 3개의 노드가 권장됨.
    # Mons는 데이터 손실이 허용되는 테스트환경의 동일한 노드에서만 허용이되어야함 (즉, 테스트환경에서만 동일한노드에 2개이상 배포해도 ok)
    allowMultiplePerNode: false
  mgr:
    modules:
    # 이 목록에 여러 모듈을 포함할 필요는 없음. "대시보드" 및 "모니터링" 모듈은 클러스터 CR의 다른 설정에 의해 이미 활성화 되어있음!
    - name: pg_autoscaler
      enabled: true
  # 클러스터 상태를 보기위해 Ceph Dashboard 활성화(enable)
  dashboard:
    enabled: true
    # 하위 경로에서 대시보드 제공 (reverse proxy를 통해 대시보드에 엑세스할 때 유용함)
    # urlPrefix: /ceph-dashboard
    # 아래 포트에서 대시보드를 제공
    # port: 8443
    # SSL을 사용하여 대시보드를 제공
    ssl: true
  # 클러스터에 대한 프로메테우스 Alert 활성화 여부
  monitoring:
    # Prometheus가 사전 설치되어 있어야함.
    enabled: false
    # prometheusRule을 배포할(deploy) 네임스페이스. 비어있는 경우, 클러스터의 네임스페이스가 사용됨.
    # 추천(recommanded):
    # 단일 Rook-Ceph 클러스터가 있는 경우, rulesNamespace를 클러스터와 동일한 네임스페이스로 설정하거나 비워둡니다.
    # 동일한 k8s클러스터에 여러개의 rook-ceph 클러스터가 있는 경우, 동일한 네임스페이스(이상적으로는 prometheus가 있는 네임스페이스)를 
    # 모든 클러스터에 대한 rulesNamespace를 설정합니다. 그렇지 않으면 중복된 alert를 받게됨.
    rulesNamespace: rook-ceph
  network:
    # enable host networking
    #provider: host
    # EXPERIMENTAL: enable the Multus network provider
    #provider: multus
    #selectors:
      # The selector keys는 `public` and `cluster`이어야함.
      # 구성에 따라 operator는 다음을 수행.
      #   1. `public` selector key 만 지정하면 public_network 및 cluster_network Ceph 설정이 해당 인터페이스에서 수신 대기
      #   2. `public`과 `cluster` selector keys 모두 지정되면 첫 번째 키는 'public_network' flag를 가리키고 두번째 키는 'cluster_network'를 가리킴
      #
      # In order to work, each selector value must match a NetworkAttachmentDefinition object in Multus
      #
      #public: public-conf --> NetworkAttachmentDefinition object name in Multus
      #cluster: cluster-conf --> NetworkAttachmentDefinition object name in Multus
    # Provide internet protocol version. IPv6, IPv4 or empty string are valid options. Empty string would mean IPv4
    #ipFamily: "IPv6"
  # enable the crash collector for ceph daemon crash collection
  crashCollector:
    disable: false
    # Uncomment daysToRetain to prune ceph crash entries older than the
    # specified number of days.
    #daysToRetain: 30
  # enable log collector, daemons will log on files and rotate
  # logCollector:
  #   enabled: true
  #   periodicity: 24h # SUFFIX may be 'h' for hours or 'd' for days.
  # automate [data cleanup process](https://github.com/rook/rook/blob/master/Documentation/ceph-teardown.md#delete-the-data-on-hosts) in cluster destruction.
  cleanupPolicy:
    # Since cluster cleanup is destructive to data, confirmation is required.
    # To destroy all Rook data on hosts during uninstall, confirmation must be set to "yes-really-destroy-data".
    # This value should only be set when the cluster is about to be deleted. After the confirmation is set,
    # Rook will immediately stop configuring the cluster and only wait for the delete command.
    # If the empty string is set, Rook will not destroy any data on hosts during uninstall.
    confirmation: ""
    # sanitizeDisks represents settings for sanitizing OSD disks on cluster deletion
    sanitizeDisks:
      # method indicates if the entire disk should be sanitized or simply ceph's metadata
      # in both case, re-install is possible
      # possible choices are 'complete' or 'quick' (default)
      method: quick
      # dataSource indicate where to get random bytes from to write on the disk
      # possible choices are 'zero' (default) or 'random'
      # using random sources will consume entropy from the system and will take much more time then the zero source
      dataSource: zero
      # iteration overwrite N times instead of the default (1)
      # takes an integer value
      iteration: 1
    # allowUninstallWithVolumes defines how the uninstall should be performed
    # If set to true, cephCluster deletion does not wait for the PVs to be deleted.
    allowUninstallWithVolumes: false
  # To control where various services will be scheduled by kubernetes, use the placement configuration sections below.
  # The example under 'all' would have all services scheduled on kubernetes nodes labeled with 'role=storage-node' and
  # tolerate taints with a key of 'storage-node'.
#  placement:
#    all:
#      nodeAffinity:
#        requiredDuringSchedulingIgnoredDuringExecution:
#          nodeSelectorTerms:
#          - matchExpressions:
#            - key: role
#              operator: In
#              values:
#              - storage-node
#      podAffinity:
#      podAntiAffinity:
#      topologySpreadConstraints:
#      tolerations:
#      - key: storage-node
#        operator: Exists
# The above placement information can also be specified for mon, osd, and mgr components
#    mon:
# Monitor deployments may contain an anti-affinity rule for avoiding monitor
# collocation on the same node. This is a required rule when host network is used
# or when AllowMultiplePerNode is false. Otherwise this anti-affinity rule is a
# preferred rule with weight: 50.
#    osd:
#    mgr:
#    cleanup:
  annotations:
#    all:
#    mon:
#    osd:
#    cleanup:
#    prepareosd:
# If no mgr annotations are set, prometheus scrape annotations will be set by default.
#    mgr:
  labels:
#    all:
#    mon:
#    osd:
#    cleanup:
#    mgr:
#    prepareosd:
  resources:
# The requests and limits set here, allow the mgr pod to use half of one CPU core and 1 gigabyte of memory
#    mgr:
#      limits:
#        cpu: "500m"
#        memory: "1024Mi"
#      requests:
#        cpu: "500m"
#        memory: "1024Mi"
# The above example requests/limits can also be added to the mon and osd components
#    mon:
#    osd:
#    prepareosd:
#    crashcollector:
#    logcollector:
#    cleanup:
  # The option to automatically remove OSDs that are out and are safe to destroy.
  removeOSDsIfOutAndSafeToRemove: false
#  priorityClassNames:
#    all: rook-ceph-default-priority-class
#    mon: rook-ceph-mon-priority-class
#    osd: rook-ceph-osd-priority-class
#    mgr: rook-ceph-mgr-priority-class
  storage: # cluster level storage configuration and selection
    useAllNodes: true
    useAllDevices: true
    #deviceFilter:
    config:
      # crushRoot: "custom-root" # specify a non-default root label for the CRUSH map
      # metadataDevice: "md0" # specify a non-rotational storage so ceph-volume will use it as block db device of bluestore.
      # databaseSizeMB: "1024" # uncomment if the disks are smaller than 100 GB
      # journalSizeMB: "1024"  # uncomment if the disks are 20 GB or smaller
      # osdsPerDevice: "1" # this value can be overridden at the node or device level
      # encryptedDevice: "true" # the default value for this option is "false"
# Individual nodes and their config can be specified as well, but 'useAllNodes' above must be set to false. Then, only the named
# nodes below will be used as storage resources.  Each node's 'name' field should match their 'kubernetes.io/hostname' label.
#    nodes:
#    - name: "172.17.4.201"
#      devices: # specific devices to use for storage can be specified for each node
#      - name: "sdb"
#      - name: "nvme01" # multiple osds can be created on high performance devices
#        config:
#          osdsPerDevice: "5"
#      - name: "/dev/disk/by-id/ata-ST4000DM004-XXXX" # devices can be specified using full udev paths
#      config: # configuration can be specified at the node level which overrides the cluster level config
#        storeType: filestore
#    - name: "172.17.4.301"
#      deviceFilter: "^sd."
  # The section for configuring management of daemon disruptions during upgrade or fencing.
  disruptionManagement:
    # If true, the operator will create and manage PodDisruptionBudgets for OSD, Mon, RGW, and MDS daemons. OSD PDBs are managed dynamically
    # via the strategy outlined in the [design](https://github.com/rook/rook/blob/master/design/ceph/ceph-managed-disruptionbudgets.md). The operator will
    # block eviction of OSDs by default and unblock them safely when drains are detected.
    managePodBudgets: true
    # A duration in minutes that determines how long an entire failureDomain like `region/zone/host` will be held in `noout` (in addition to the
    # default DOWN/OUT interval) when it is draining. This is only relevant when  `managePodBudgets` is `true`. The default value is `30` minutes.
    osdMaintenanceTimeout: 30
    # A duration in minutes that the operator will wait for the placement groups to become healthy (active+clean) after a drain was completed and OSDs came back up.
    # Operator will continue with the next drain if the timeout exceeds. It only works if `managePodBudgets` is `true`.
    # No values or 0 means that the operator will wait until the placement groups are healthy before unblocking the next drain.
    pgHealthCheckTimeout: 0
    # If true, the operator will create and manage MachineDisruptionBudgets to ensure OSDs are only fenced when the cluster is healthy.
    # Only available on OpenShift.
    manageMachineDisruptionBudgets: false
    # Namespace in which to watch for the MachineDisruptionBudgets.
    machineDisruptionBudgetNamespace: openshift-machine-api

  # healthChecks
  # Valid values for daemons are 'mon', 'osd', 'status'
  healthCheck:
    daemonHealth:
      mon:
        disabled: false
        interval: 45s
      osd:
        disabled: false
        interval: 60s
      status:
        disabled: false
        interval: 60s
    # Change pod liveness probe, it works for all mon,mgr,osd daemons
    livenessProbe:
      mon:
        disabled: false
      mgr:
        disabled: false
      osd:
        disabled: false
